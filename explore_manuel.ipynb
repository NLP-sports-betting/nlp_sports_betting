{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e08f262-8eb9-48fe-a10d-36c4c82df529",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71252573-1d79-4ab4-b6bd-de42eced80f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "from pprint import pprint \n",
    "import time\n",
    "import json\n",
    "\n",
    "import acquire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "358db831-d458-49b1-98f8-08669b9bf6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#standard imports\n",
    "import numpy as np\n",
    "\n",
    "#import\n",
    "import unicodedata\n",
    "#import regular expression operations\n",
    "import re\n",
    "#import natural language toolkit\n",
    "import nltk\n",
    "#import stopwords list\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0db947c7-aabd-497e-9a86-7472088ea13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80033dd9-d56f-4f6b-950e-4835f7abbecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db988ed9-3543-49cb-af0f-156b075418ab",
   "metadata": {},
   "source": [
    "#### Acquire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca611fa-c47c-4542-9e5f-e404ed3448b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = 'https://github.com/search?o=desc&q=sports+betting&s=stars&type=Repositories&p=1'\n",
    "\n",
    "# response = get(url)\n",
    "# response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c3e79d-04b8-48be-89d9-74d76550c37c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# soup = BeautifulSoup(response.content, 'html.parser')\n",
    "# soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714ce062-ed00-4852-b24a-7399d6c6f8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repos = soup.find_all('a', class_=\"v-align-middle\")\n",
    "# repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4b3cdc-1f65-4465-bcce-186365a8eed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the links into a list\n",
    "# link_list = [link['href'] for link in repos]\n",
    "# link_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df724cdd-f625-476b-a6d4-45dc59678884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['https://github.com'+link['href'] for link in repos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe59162-cc72-43ea-a64b-78aef18f8f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = 'https://github.com/search?o=desc&q=sports+betting&s=stars&type=Repositories&p='"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bc6429-1187-46ba-bb69-aaed551968e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# link_list = []\n",
    "\n",
    "# for x in range(1,13):\n",
    "#     # print(x)\n",
    "#     print(url+str(x))\n",
    "#     response = get(url+str(x))\n",
    "#     soup = BeautifulSoup(response.content, 'html.parser')\n",
    "#     repos = soup.find_all('a', class_=\"v-align-middle\")\n",
    "#     # print(repos)\n",
    "#     time.sleep(10)\n",
    "#     print(len(repos))\n",
    "#     link_list.extend([link['href'] for link in repos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094bd973-beb2-4206-b34e-ad89c16de56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(link_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac93ea49-ffc6-45d3-86d2-1a92dc77bbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa726ce-72d9-4e41-a79a-7820997b99dd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data = acquire.scrape_github_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992f3d00-bdbb-43c0-bb3a-7c2ec29a12e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# json.dump(data, open(\"data.json\", \"w\"), indent=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31cb47f-15fe-4d4e-acba-dcbc696abaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f341046d-3641-4551-b530-754609787a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a86f40c-7c32-4fcb-a4cd-d560c8bc554e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.readme_contents = df.readme_contents.str.replace('[/,_,-,:,\"]', ' ', regex=True)\n",
    "\n",
    "df.readme_contents = df.readme_contents.str.replace('heavy', '').str.replace('check', '').str.replace('mark', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98331641-8984-4e79-8db4-42d8e9a3263c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JavaScript           24\n",
       "Python               23\n",
       "Jupyter Notebook     10\n",
       "HTML                  8\n",
       "R                     8\n",
       "TypeScript            6\n",
       "Ruby                  5\n",
       "PHP                   4\n",
       "C#                    4\n",
       "Solidity              2\n",
       "Rust                  2\n",
       "Visual Basic .NET     1\n",
       "Go                    1\n",
       "Swift                 1\n",
       "Java                  1\n",
       "Name: language, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.language.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444d0e10-f8d8-4c42-a6e7-9aacc2d76ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(original):\n",
    "    '''\n",
    "    Input: original text or .apply(basic_clean) to entire data frame\n",
    "    Actions: \n",
    "    lowercase everything,\n",
    "    normalizes everything,\n",
    "    removes anything that's not a letter, number, whitespace, or single quote\n",
    "    Output: Cleaned text\n",
    "    '''\n",
    "    # lowercase everything\n",
    "    basic_cleaned = original.lower()\n",
    "    # normalize unicode characters\n",
    "    basic_cleaned = unicodedata.normalize('NFKD', basic_cleaned)\\\n",
    "    .encode('ascii', 'ignore')\\\n",
    "    .decode('utf-8')\n",
    "    # Replace anything that is not a letter, number, whitespace or a single quote.\n",
    "    basic_cleaned = re.sub(r'[^a-z0-9\\'\\s]', '', basic_cleaned)\n",
    "    \n",
    "    return basic_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db732ee3-f9d2-4af6-b766-0265f4486372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(basic_cleaned):\n",
    "    '''\n",
    "    Input: basic_cleaned text string or .apply(tokenize) to entire data frame\n",
    "    Actions:\n",
    "    creates the tokenizer\n",
    "    uses the tokenizer\n",
    "    Output: clean_tokenize text string\n",
    "    '''\n",
    "    #create the tokenizer\n",
    "    tokenize = nltk.tokenize.ToktokTokenizer()\n",
    "    #use the tokenizer\n",
    "    clean_tokenize = tokenize.tokenize(basic_cleaned, return_str=True)\n",
    "    \n",
    "    return clean_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bce8dd-4456-4020-8019-4e149deeedd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(lemma_or_stem, extra_words=[], exclude_words=[]):\n",
    "    '''\n",
    "    Input:text string or .apply(remove_stopwords) to entire data frame\n",
    "    Action: removes standard stop words\n",
    "    Output: parsed_article\n",
    "    '''\n",
    "    # save stopwords\n",
    "    stopwords_ls = stopwords.words('english')\n",
    "    # removing any stopwords in exclude list\n",
    "    stopwords_ls = set(stopwords_ls) - set(exclude_words)\n",
    "    # adding any stopwords in extra list\n",
    "    stopwords_ls = stopwords_ls.union(set(extra_words))\n",
    "    \n",
    "    # split words in article\n",
    "    words = lemma_or_stem.split()\n",
    "    # remove stopwords from list of words\n",
    "    filtered = [word for word in words if word not in stopwords_ls]\n",
    "    # join words back together\n",
    "    parsed_article = ' '.join(filtered)\n",
    "    \n",
    "    return parsed_article\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce5bc48-814b-4285-a7a6-4e5ecdd115ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(clean_tokenize):\n",
    "    '''\n",
    "    Inputs: clean_tokenize\n",
    "    Actions: creates lemmatizer and applies to each word\n",
    "    Outputs: clean_tokenize_lemma\n",
    "    '''\n",
    "    #create the lemmatizer\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    #use lemmatize - apply to each word in our string\n",
    "    lemmas = [wnl.lemmatize(word) for word in clean_tokenize.split()]\n",
    "    #join words back together\n",
    "    clean_tokenize_lemma = ' '.join(lemmas)\n",
    "    \n",
    "    return clean_tokenize_lemma\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676217ad-79a7-462c-8b58-a360454bf0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    '''\n",
    "    A simple function to cleanup text data.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to be cleaned.\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of lemmatized words after cleaning.\n",
    "    '''\n",
    "    \n",
    "    # basic_clean() function from last lesson:\n",
    "    # Normalize text by removing diacritics, encoding to ASCII, decoding to UTF-8, and converting to lowercase\n",
    "    text = (unicodedata.normalize('NFKD', text)\n",
    "             .encode('ascii', 'ignore')\n",
    "             .decode('utf-8', 'ignore')\n",
    "             .lower())\n",
    "    \n",
    "    # Remove punctuation, split text into words\n",
    "    words = re.sub(r'[^\\w\\s]', '', text).split()\n",
    "    \n",
    "    \n",
    "    # lemmatize() function from last lesson:\n",
    "    # Initialize WordNet lemmatizer\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    \n",
    "    # Combine standard English stopwords with additional stopwords\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    \n",
    "    # Lemmatize words and remove stopwords\n",
    "    cleaned_words = [wnl.lemmatize(word) for word in words if word not in stopwords]\n",
    "    \n",
    "    return cleaned_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792ac050-7674-4c38-ac03-77e1610b7c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.rename(columns={'readme_contents': 'original'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523b31db-226e-4615-8fca-0b6a4d76fabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['clean_norm_token'] = df.readme_contents.apply(tokenize).apply(basic_clean).apply(remove_stopwords).apply(lemmatize)\n",
    "\n",
    "\n",
    "df.clean_norm_token = df.clean_norm_token.str.replace('124', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7038ec8-16e7-4245-8644-a78a20eb70b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['JavaScript', 'Python', 'Jupyter Notebook', 'HTML', 'R']\n"
     ]
    }
   ],
   "source": [
    "#in language column replace language with other if it is not in the top 3 languages\n",
    "top_5 = df.language.value_counts().head(5).index.tolist()\n",
    "print(top_5)\n",
    "df.language = df.language.apply(lambda x: x if x in top_5 else 'other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd86b26-dec9-45af-ad97-6d5f702e4bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.language.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ada4299-1dd1-4204-be6f-6907564bed8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#baseline is 43%\n",
    "df.language.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "993fbc8a-630a-40b5-8342-d629699fcd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wrangle as w\n",
    "import auto_model2a1 as atm\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f3e8781-1040-45c8-a333-013534561aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calls the split function from wrangle.py\n",
    "train, validate, test = w.split_function_cat_target(df, \"language\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39cece26-de25-492f-95de-791cecfeecae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo</th>\n",
       "      <th>language</th>\n",
       "      <th>readme_contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/kyleskom/NBA-Machine-Learning-Sports-Betting</td>\n",
       "      <td>Python</td>\n",
       "      <td># NBA Sports Betting Using Machine Learning üèÄ\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/pretrehr/Sports-betting</td>\n",
       "      <td>Python</td>\n",
       "      <td>[![forthebadge made-with-python](http://ForThe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/llSourcell/ChatGPT_Sports_Betting_Bot</td>\n",
       "      <td>Jupyter Notebook</td>\n",
       "      <td># ChatGPT Sports Betting Bot\\n\\nThis is the co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/georgedouzas/sports-betting</td>\n",
       "      <td>Python</td>\n",
       "      <td>[scikit-learn]: &lt;http://scikit-learn.org/stabl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/sedemmler/WagerBrain</td>\n",
       "      <td>Python</td>\n",
       "      <td># WagerBrain\\nA package containing the essenti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            repo          language  \\\n",
       "0  /kyleskom/NBA-Machine-Learning-Sports-Betting            Python   \n",
       "1                       /pretrehr/Sports-betting            Python   \n",
       "2         /llSourcell/ChatGPT_Sports_Betting_Bot  Jupyter Notebook   \n",
       "3                   /georgedouzas/sports-betting            Python   \n",
       "4                          /sedemmler/WagerBrain            Python   \n",
       "\n",
       "                                     readme_contents  \n",
       "0  # NBA Sports Betting Using Machine Learning üèÄ\\...  \n",
       "1  [![forthebadge made-with-python](http://ForThe...  \n",
       "2  # ChatGPT Sports Betting Bot\\n\\nThis is the co...  \n",
       "3  [scikit-learn]: <http://scikit-learn.org/stabl...  \n",
       "4  # WagerBrain\\nA package containing the essenti...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c57f001d-6b7e-4f47-911a-a5c82b1bddb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo</th>\n",
       "      <th>language</th>\n",
       "      <th>readme_contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>/valenIndovino/apuestas-deportivas</td>\n",
       "      <td>HTML</td>\n",
       "      <td># ApuestasOficial\\nDesarrollo del Proyecto: Pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>/JDaniloC/BetBot</td>\n",
       "      <td>HTML</td>\n",
       "      <td># BetBot\\n\\n![BetBot Video](./docs/images/vide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>/addtek/reactnative_sports_betting_app</td>\n",
       "      <td>JavaScript</td>\n",
       "      <td># reactnative_sports_betting_app\\nA Sport bett...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>/mautomic/NitrogenSports-Analysis</td>\n",
       "      <td>Python</td>\n",
       "      <td>## NitrogenSports-Analysis\\n\\nThis script is m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>/dashee87/betScrapeR</td>\n",
       "      <td>R</td>\n",
       "      <td>---\\r\\ntitle: \"README\"\\r\\noutput:\\r\\n  md_docu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      repo    language  \\\n",
       "62      /valenIndovino/apuestas-deportivas        HTML   \n",
       "13                        /JDaniloC/BetBot        HTML   \n",
       "26  /addtek/reactnative_sports_betting_app  JavaScript   \n",
       "28       /mautomic/NitrogenSports-Analysis      Python   \n",
       "16                    /dashee87/betScrapeR           R   \n",
       "\n",
       "                                      readme_contents  \n",
       "62  # ApuestasOficial\\nDesarrollo del Proyecto: Pa...  \n",
       "13  # BetBot\\n\\n![BetBot Video](./docs/images/vide...  \n",
       "26  # reactnative_sports_betting_app\\nA Sport bett...  \n",
       "28  ## NitrogenSports-Analysis\\n\\nThis script is m...  \n",
       "16  ---\\r\\ntitle: \"README\"\\r\\noutput:\\r\\n  md_docu...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13fd5e8-ce2b-48bc-a56f-18ccbbde0b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a988de-f2f5-4e87-b8a0-b03a0f2a13df",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_words = w.clean(' '.join(train[train.language=='Python']['lemmatized']))\n",
    "java_script_words = w.clean(' '.join(train[train.language=='JavaScript']['lemmatized']))\n",
    "jupyter_notebook_words = w.clean(' '.join(train[train.language=='Jupyter Notebook']['lemmatized']))\n",
    "html_words = w.clean(' '.join(train[train.language=='HTML']['lemmatized']))\n",
    "r_words = w.clean(' '.join(train[train.language=='R']['lemmatized']))\n",
    "other_words = w.clean(' '.join(train[train.language=='other']['lemmatized']))\n",
    "\n",
    "all_words = w.clean(' '.join(df['lemmatized']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1aa7762-a2ab-43ab-8274-f2f0fda0bdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_freq = pd.Series(python_words).value_counts()\n",
    "java_script_freq = pd.Series(java_script_words).value_counts()\n",
    "jupyter_notebook_freq = pd.Series(jupyter_notebook_words).value_counts()\n",
    "html_freq = pd.Series(html_words).value_counts()\n",
    "r_freq = pd.Series(r_words).value_counts()\n",
    "other_freq = pd.Series(other_words).value_counts()\n",
    "\n",
    "all_freq = pd.Series(all_words).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663921be-907e-410f-84ec-1c3b859ec95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ngrams(words, n):\n",
    "    return pd.Series(nltk.ngrams(words, n)).value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b56ca9-9ac1-4238-9372-115fc269ff94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bigrams(words):\n",
    "    word_data = {k[0] + ' ' + k[1]: v for k, v in words.to_dict().items()}\n",
    "    \n",
    "    word_img = WordCloud(background_color='white', width=800, height=400).generate_from_frequencies(word_data)\n",
    "    \n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.imshow(word_img)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c4ea8b-17b8-47d9-89f6-47a20c46596d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting basic style parameters for matplotlib\n",
    "plt.rc('figure', figsize=(13, 7))\n",
    "plt.style.use('seaborn-darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba290425-b3f1-4cdd-8e47-03a8017caa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets combine all 3 dfs horizontally\n",
    "# fill any missing values with zero\n",
    "# convert the resulting df to contain only integer values:\n",
    "\n",
    "word_counts = pd.concat([python_freq, java_script_freq, jupyter_notebook_freq,\n",
    "                        html_freq, r_freq, all_freq], axis=1).fillna(0).astype(int)\n",
    "\n",
    "# rename the col names\n",
    "word_counts.columns = ['python', 'java_script', 'jupyter_notebook', 'html', 'r', 'all']\n",
    "\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96032447-3b7a-49e0-a673-9564c17bbb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(nltk.ngrams(python_words,2)).value_counts().head(20).plot.barh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9123880a-7c09-4075-b942-2f3c45965f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "blog_img = WordCloud(background_color='white').generate_from_frequencies(python_freq)\n",
    "plt.imshow(blog_img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9fcddf-9d12-4612-a699-f1d1fa241da8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d92c58-ede1-4803-a450-c44ffafce638",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0ec9c4-af9a-4fa3-b3c1-d8fc41a9fd39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5459cb8b-22a3-445f-bccf-0463b4abcf24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb9deb7-ca43-49ad-93a5-ea86b99f329f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed16d034-27db-490a-b7ec-5ead510cb5a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1f919c-27a4-4f3f-8f73-7159026cde43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a515bf-4301-48b9-a67b-5821e737dac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.original = df.original.str.replace('[/,_,-,:,\"]', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87dca16-1b4e-4967-a849-276444d37674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eac8a32-1e9d-4d22-87d2-f26d069b6645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.reset_option(\"display.max_colwidth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80fe004-2370-41d2-b1ea-ab8374d09fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.language.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f34598-b85b-4d5d-b9a0-b8dd4170e553",
   "metadata": {},
   "outputs": [],
   "source": [
    "blog_img = WordCloud(background_color='white').generate_from_frequencies(java_script_freq)\n",
    "plt.imshow(blog_img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11a2080-26fc-4e30-8370-1de39a9e419c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6876988-e284-4deb-88e7-c59a5c99b8f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3e54eb-ac19-4001-91ad-4b55231be2b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba93b70-6112-49de-ac60-a2ec70e86617",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c19629-16e4-4878-b595-f57d6b3c9367",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sets the X and y variables for train, validate, and test\n",
    "X_train = train.clean_norm_token\n",
    "y_train = train.language\n",
    "X_validate = validate.clean_norm_token\n",
    "y_validate = validate.language\n",
    "X_test = test.clean_norm_token\n",
    "y_test = test.language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34506b12-ca04-44ee-a50c-bd8fbd4778c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c462b3-3676-46d7-aef0-3b151d530e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make my bag of words Term Frequency \n",
    "cv = CountVectorizer()\n",
    "X_bow = cv.fit_transform(X_train) \n",
    "X_validate_bow = cv.transform(X_validate)\n",
    "X_test_bow = cv.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8be9a0-195b-4e71-906f-01a40023deb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_validate_bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca0fafd-9704-49d5-8b17-0292fa933139",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c185839-2f00-4786-8580-40e55c9c16fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#auto runs through knn models with k values 5-20\n",
    "atm.auto_knn_scores(X_bow, y_train, X_validate_bow, y_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ca3b3c-2b3c-49e8-ad51-83738b2f1837",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_nearest2(X_train_scaled, y_train, X_validate_scaled, y_validate):\n",
    "    \"\"\"\n",
    "    Trains and evaluates KNN models for different values of k and plots the results.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train: array-like, shape (n_samples, n_features)\n",
    "        Training input samples.\n",
    "    y_train: array-like, shape (n_samples,)\n",
    "        Target values for the training input samples.\n",
    "    X_validate: array-like, shape (n_samples, n_features)\n",
    "        Validation input samples.\n",
    "    y_validate: array-like, shape (n_samples,)\n",
    "        Target values for the validation input samples.\n",
    "    Returns:\n",
    "    --------\n",
    "    results: pandas DataFrame\n",
    "        Contains the train and validation accuracy for each value of k.\n",
    "    \"\"\"\n",
    "    metrics = []\n",
    "    train_score = []\n",
    "    validate_score = []\n",
    "    for k in range(1,21):\n",
    "        knn = KNeighborsClassifier(n_neighbors=k)\n",
    "        knn.fit(X_train_scaled, y_train)\n",
    "        train_score.append(knn.score(X_train_scaled, y_train))\n",
    "        validate_score.append(knn.score(X_validate_scaled, y_validate))\n",
    "        diff_score = train_score[-1] - validate_score[-1]\n",
    "        metrics.append({'k': k, 'train_score': train_score[-1], 'validate_score': validate_score[-1], 'diff_score': diff_score})\n",
    "    baseline_accuracy = y_train.value_counts().max() / y_train.value_counts().sum()\n",
    "    results = pd.DataFrame.from_records(metrics)\n",
    "    # modify the last few lines of the function\n",
    "    # drop the diff_score column before plotting\n",
    "    results_for_plotting = results.drop(columns=['diff_score'])\n",
    "    with sns.axes_style('whitegrid'):\n",
    "        ax = results_for_plotting.set_index('k').plot(figsize=(16,9))\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.axhline(baseline_accuracy, linewidth=2, color='black', label='baseline')\n",
    "    plt.xticks(np.arange(0,21,1))\n",
    "    min_diff_idx = np.abs(results['diff_score']).argmin()\n",
    "    min_diff_k = results.loc[min_diff_idx, 'k']\n",
    "    min_diff_score = results.loc[min_diff_idx, 'diff_score']\n",
    "    ax.axvline(min_diff_k, linestyle='--', linewidth=2, color='red', label=f'min diff at k={min_diff_k} (diff={min_diff_score:.3f})')\n",
    "    plt.fill_between(results['k'], train_score, validate_score, alpha=0.2, color='gray', where=(results['k'] > 0))\n",
    "    plt.title('K Nearest Neighbor', fontsize=18)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c44680-fd5c-4a98-b51a-1cc344f256fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts().max() / y_train.value_counts().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6073ec25-4566-4b5a-a95f-9ef6c0861c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_nearest2(X_bow, y_train, X_validate_bow, y_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c29290-74ca-408b-9db2-e3885eda5468",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917c02ca-2ff5-4247-8fc3-8d0a86dcb287",
   "metadata": {},
   "outputs": [],
   "source": [
    "#auto runs through random forest models with max_depth values 5-20\n",
    "atm.auto_random_forest_scores(X_bow, y_train, X_validate_bow, y_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bf8b1c-d034-430f-9694-96728ecea080",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_scores(X_train_scaled, y_train, X_validate_scaled, y_validate):\n",
    "    \"\"\"\n",
    "    Trains and evaluates a random forest classifier with different combinations of hyperparameters. The function takes in\n",
    "    training and validation datasets, and returns a dataframe summarizing the model performance on each combination of\n",
    "    hyperparameters.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : pandas DataFrame\n",
    "        Features of the training dataset.\n",
    "    y_train : pandas Series\n",
    "        Target variable of the training dataset.\n",
    "    X_validate : pandas DataFrame\n",
    "        Features of the validation dataset.\n",
    "    y_validate : pandas Series\n",
    "        Target variable of the validation dataset.\n",
    "    Returns:\n",
    "    --------\n",
    "    df : pandas DataFrame\n",
    "        A dataframe summarizing the model performance on each combination of hyperparameters.\n",
    "    \"\"\"\n",
    "    #define variables\n",
    "    train_scores = []\n",
    "    validate_scores = []\n",
    "    min_samples_leaf_values = [1, 2, 3, 4, 5, 6, 7, 8 , 9, 10]\n",
    "    max_depth_values = [10, 9, 8, 7, 6, 5, 4, 3, 2, 1]\n",
    "    for min_samples_leaf, max_depth in zip(min_samples_leaf_values, max_depth_values):\n",
    "        rf = RandomForestClassifier(min_samples_leaf=min_samples_leaf, max_depth=max_depth,random_state=123)\n",
    "        rf.fit(X_train_scaled, y_train)\n",
    "        train_score = rf.score(X_train_scaled, y_train)\n",
    "        validate_score = rf.score(X_validate_scaled, y_validate)\n",
    "        train_scores.append(train_score)\n",
    "        validate_scores.append(validate_score)\n",
    "    # Calculate the difference between the train and validation scores\n",
    "    diff_scores = [train_score - validate_score for train_score, validate_score in zip(train_scores, validate_scores)]\n",
    "    #Put results into a dataframe\n",
    "    df = pd.DataFrame({\n",
    "        'min_samples_leaf': min_samples_leaf_values,\n",
    "        'max_depth': max_depth_values,\n",
    "        'train_score': train_scores,\n",
    "        'validate_score': validate_scores,\n",
    "        'diff_score': diff_scores})\n",
    "    # Set plot style\n",
    "    sns.set_style('whitegrid')\n",
    "    # Create plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(max_depth_values, train_scores, label='train', marker='o', color='blue')\n",
    "    plt.plot(max_depth_values, validate_scores, label='validation', marker='o', color='orange')\n",
    "    plt.fill_between(max_depth_values, train_scores, validate_scores, alpha=0.2, color='gray')\n",
    "    plt.xticks([2,4,6,8,10],['Leaf 9 and Depth 2','Leaf 7 and Depth 4','Leaf 5 and Depth 6','Leaf 3 and Depth 8','Leaf 1and Depth 10'], rotation = 45)\n",
    "    plt.xlabel('min_samples_leaf and max_depth', fontsize=14)\n",
    "    plt.ylabel('Accuracy', fontsize=14)\n",
    "    plt.title('Random Forest Classifier Performance', fontsize=18)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.show()\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c9f8dd-cb23-4f8d-8c2b-a75aa37995b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_scores(X_bow, y_train, X_validate_bow, y_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612725a6-b761-4114-89b7-52975ef1df0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#auto runs through decision tree models with the \"entropy\" parameter with max_depth values 5-20\n",
    "atm.auto_random_forest_entropy_scores(X_bow, y_train, X_validate_bow, y_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7475369-15ad-491e-93bf-998361031a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#auto runs through logistic regression models with C values .1-1\n",
    "atm.auto_lo_regress_scores(X_bow, y_train, X_validate_bow, y_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dbc658-c655-4f08-abfb-41349f3bcb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make my bag of words TF-IDF\n",
    "tfidf = TfidfVectorizer()\n",
    "X_bow = tfidf.fit_transform(X_train) \n",
    "X_validate_bow = tfidf.transform(X_validate)\n",
    "X_test_bow = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a702d2-d255-4314-88c1-125a022b9e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#auto runs through knn models with k values 5-20\n",
    "atm.auto_knn_scores(X_bow, y_train, X_validate_bow, y_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ed4fa6-e673-4666-8674-3a0d8d036bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#auto runs through random forest models with max_depth values 5-20\n",
    "atm.auto_random_forest_scores(X_bow, y_train, X_validate_bow, y_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e2aa60-363a-4a71-87a3-55e676f58e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#auto runs through decision tree models with the \"entropy\" parameter with max_depth values 5-20\n",
    "atm.auto_random_forest_entropy_scores(X_bow, y_train, X_validate_bow, y_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49cf8d7-8814-449f-acf3-4d8658df84e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#auto runs through logistic regression models with C values .1-1\n",
    "atm.auto_lo_regress_scores(X_bow, y_train, X_validate_bow, y_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f285fb9d-c00e-4207-8f46-e530421896e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c5d497-3535-4b56-a3e3-3819ac3707d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
